import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
import random

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from sklearn.metrics import classification_report
data = np.load("candidate_dataset.npz")

print("Keys in dataset:", data.files)
x_train = data["x_train"]
y_train = data["y_train"]

x_val = data["x_val"]
y_val = data["y_val"]

print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)
print("x_val shape:", x_val.shape)
print("y_val shape:", y_val.shape)
y_train = y_train.flatten()
y_val = y_val.flatten()

print("Fixed y_train shape:", y_train.shape)
print("Fixed y_val shape:", y_val.shape)
if x_train.ndim == 4:
    x_train = np.mean(x_train, axis=-1)
    x_val = np.mean(x_val, axis=-1)

print("After grayscale conversion:")
print("x_train:", x_train.shape)
print("x_val:", x_val.shape)
print("Train label distribution (noisy):", Counter(y_train))
print("Val label distribution (clean):", Counter(y_val))

plt.bar(Counter(y_train).keys(), Counter(y_train).values())
plt.title("Training Label Distribution (Noisy)")
plt.show()

plt.bar(Counter(y_val).keys(), Counter(y_val).values())
plt.title("Validation Label Distribution (Clean)")
plt.show()
fig, axes = plt.subplots(7, 5, figsize=(10,12))

for cls in range(7):
    idxs = np.where(y_train == cls)[0]
    samples = random.sample(list(idxs), min(5, len(idxs)))

    for i, idx in enumerate(samples):
        axes[cls, i].imshow(x_train[idx], cmap="gray")
        axes[cls, i].axis("off")
        axes[cls, i].set_title(f"Class {cls}")

plt.tight_layout()
plt.show()
class DermDataset(Dataset):
    def __init__(self, images, labels):
        self.images = images.astype(np.float32) / 255.0
        self.labels = labels.astype(np.int64)

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = torch.tensor(self.images[idx]).unsqueeze(0)  
        label = torch.tensor(self.labels[idx])
        return image, label
train_dataset = DermDataset(x_train, y_train)
val_dataset = DermDataset(x_val, y_val)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
class CNN(nn.Module):
    def __init__(self, num_classes=7):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2,2)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        return self.fc2(x)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = CNN(num_classes=7).to(device)
class LabelSmoothingLoss(nn.Module):
    def __init__(self, classes=7, smoothing=0.1):
        super().__init__()
        self.classes = classes
        self.smoothing = smoothing

    def forward(self, pred, target):
        log_probs = F.log_softmax(pred, dim=1)

        with torch.no_grad():
            true_dist = torch.zeros_like(log_probs)
            true_dist.fill_(self.smoothing / (self.classes - 1))
            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)

        return torch.mean(torch.sum(-true_dist * log_probs, dim=1))
criterion = LabelSmoothingLoss(classes=7, smoothing=0.1)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
def evaluate(model, loader):
    model.eval()
    correct, total = 0, 0

    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return correct / total
best_val_acc = 0.0
patience = 5
counter = 0

train_losses = []
val_accuracies = []

for epoch in range(30):
    model.train()
    running_loss = 0.0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(train_loader)
    val_acc = evaluate(model, val_loader)

    train_losses.append(avg_loss)
    val_accuracies.append(val_acc)

    print(f"Epoch {epoch+1:02d} | Train Loss: {avg_loss:.4f} | Val Acc: {val_acc:.4f}")

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        counter = 0
        torch.save(model.state_dict(), "best_model.pth")
    else:
        counter += 1

    if counter >= patience:
        print("Early stopping triggered")
        break
plt.plot(train_losses, label="Train Loss")
plt.plot(val_accuracies, label="Val Accuracy")
plt.legend()
plt.title("Training Logs")
plt.show()
model.load_state_dict(torch.load("best_model.pth"))
model.eval()

y_true, y_pred = [], []

with torch.no_grad():
    for images, labels in val_loader:
        images = images.to(device)
        outputs = model(images)
        preds = outputs.argmax(dim=1).cpu().numpy()
        y_pred.extend(preds)
        y_true.extend(labels.numpy())

print(classification_report(y_true, y_pred))
torch.save(model.state_dict(), "best_model.pth")
